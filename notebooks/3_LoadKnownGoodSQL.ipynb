{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Open Data QnA: Cache Known Good Queries in Vector Store**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook shows how to cahce known good queries in a Vector Store that has already been set up using [1_SetUpVectorStore.ipynb](1_SetUpVectorStore.ipynb). The queries are loaded into the vector store from the csv files (/scripts/known_good_sql.csv)\n",
    "\n",
    "Supported vector stores: \n",
    "- pgvector on PostgreSQL \n",
    "- BigQuery vector\n",
    "\n",
    "\n",
    "The notebook covers the following steps: \n",
    "> 1. Clean an existing embeddings table for known good queries (if loading_mode = 'refresh')\n",
    "\n",
    "> 2. Add known good queries from csv file to the embeddings table in the vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöß **0. Pre-requisites**\n",
    "\n",
    "Make sure that you have completed the intial setup process using [1_SetUpVectorStore.ipynb](1_SetUpVectorStore.ipynb). If the 1_SetUpVectorStore notebook has been run successfully, the following are set up:\n",
    "* GCP project and all the required IAM permissions\n",
    "\n",
    "* **Environment to run the solution**\n",
    "\n",
    "* Data source and Vector store for the solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è **1. Retrieve Configuration Parameters**\n",
    "The notebook will load all the configuration parameters from the `config.ini` file in the root directory. \n",
    "Most of these parameters were set in the initial notebook `1_SetUpVectorStore.ipynb` and save to the 'config.ini file.\n",
    "Use the below cells to retrieve these values and specify additional ones required for this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(module_path+'/config.ini')\n",
    "\n",
    "PROJECT_ID = config['GCP']['PROJECT_ID']\n",
    "VECTOR_STORE = config['CONFIG']['VECTOR_STORE']\n",
    "PG_DATABASE = config['PGCLOUDSQL']['PG_DATABASE']\n",
    "PG_USER = config['PGCLOUDSQL']['PG_USER']\n",
    "PG_REGION = config['PGCLOUDSQL']['PG_REGION'] \n",
    "PG_INSTANCE = config['PGCLOUDSQL']['PG_INSTANCE'] \n",
    "PG_PASSWORD = config['PGCLOUDSQL']['PG_PASSWORD']\n",
    "BQ_OPENDATAQNA_DATASET_NAME = config['BIGQUERY']['BQ_OPENDATAQNA_DATASET_NAME']\n",
    "BQ_LOG_TABLE_NAME = config['BIGQUERY']['BQ_LOG_TABLE_NAME'] \n",
    "BQ_DATASET_REGION = config['BIGQUERY']['BQ_DATASET_REGION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîê **2. Authenticate and Connect to Google Cloud Project**\n",
    "Authenticate to Google Cloud as the IAM user logged into this notebook in order to access your Google Cloud Project.\n",
    "\n",
    "You can do this within Google Colab or using the Application Default Credentials in the Google Cloud CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Colab Auth\"\"\" \n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "\n",
    "\n",
    "\"\"\"Google CLI Auth\"\"\"\n",
    "# !gcloud auth application-default login\n",
    "\n",
    "\n",
    "import google.auth\n",
    "import os\n",
    "\n",
    "credentials, project_id = google.auth.default()\n",
    "\n",
    "os.environ['GOOGLE_CLOUD_QUOTA_PROJECT']=PROJECT_ID\n",
    "os.environ['GOOGLE_CLOUD_PROJECT']=PROJECT_ID\n",
    "\n",
    "# Configure gcloud.\n",
    "print(f'Project has been set to {PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ **3. Cache Knwon Good Queries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format of the Known Good SQL File (known_good_sql.csv)\n",
    "\n",
    "prompt | sql | user_grouping [3 columns]\n",
    "\n",
    "prompt ==> Natural Language Question corresponding to query\n",
    "\n",
    "sql ==> SQL for the user question (Note that the sql should enclosed in quotes and only in single line. Please remove the line  break)\n",
    "\n",
    "user_grouping ==>This name should exactly  match the user_grouping for Postgres Source or Big Query as data_source_list.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the csv file and load as dataframe\n",
    "import pandas as pd\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "root_dir = os.path.expanduser('~')  # Start at the user's home directory\n",
    "\n",
    "while current_dir != root_dir:\n",
    "    for dirpath, dirnames, filenames in os.walk(current_dir):\n",
    "        config_path = os.path.join(dirpath, 'known_good_sql.csv')\n",
    "        if os.path.exists(config_path):\n",
    "            file_path = config_path  # Update root_dir to the found directory\n",
    "            break  # Stop outer loop once found\n",
    "\n",
    "    current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "print(\"Known Good SQL Found at Path :: \"+file_path)\n",
    "\n",
    "# Load the file\n",
    "df_kgq = pd.read_csv(file_path)\n",
    "df_kgq = df_kgq.loc[:, [\"prompt\", \"sql\", \"user_grouping\"]]\n",
    "df_kgq = df_kgq.dropna()\n",
    "print(df_kgq)\n",
    "\n",
    "print('Known Good SQLs Loaded into a Dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify mode for loading the known good sql\n",
    "\n",
    "The known good sql can loaded in two modes:\n",
    "* Append mode: Apended to the existing KGQ in the vector store \n",
    "* Refresh mode: Delete the existing KGQ and create of fresh copy from KGQ in known_good_sql.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading_mode = 'refresh' # Options 'append' or 'refresh'\n",
    "assert loading_mode in {'append', 'refresh'}, \"‚ö†Ô∏è Invalid loading_mode. Must be 'append' and 'refresh'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have Known Good Queries, load them to known_good_sql.csv file; \n",
    "# These will be used as few shot examples for query generation. \n",
    "\n",
    "from embeddings.kgq_embeddings import setup_kgq_table, store_kgq_embeddings\n",
    "\n",
    "if loading_mode == 'refresh':\n",
    "    # Delete any old tables and create a new table to KGQ embeddings\n",
    "    if VECTOR_STORE=='bigquery-vector':\n",
    "        await(setup_kgq_table(project_id=PROJECT_ID,\n",
    "                            instance_name=None,\n",
    "                            database_name=None,\n",
    "                            schema=BQ_OPENDATAQNA_DATASET_NAME,\n",
    "                            database_user=None,\n",
    "                            database_password=None,\n",
    "                            region=BQ_DATASET_REGION,\n",
    "                            VECTOR_STORE = VECTOR_STORE\n",
    "                            ))\n",
    "\n",
    "    elif VECTOR_STORE=='cloudsql-pgvector':\n",
    "        await(setup_kgq_table(project_id=PROJECT_ID,\n",
    "                            instance_name=PG_INSTANCE,\n",
    "                            database_name=PG_DATABASE,\n",
    "                            schema=None,\n",
    "                            database_user=PG_USER,\n",
    "                            database_password=PG_PASSWORD,\n",
    "                            region=PG_REGION,\n",
    "                            VECTOR_STORE = VECTOR_STORE\n",
    "                            ))\n",
    "\n",
    "\n",
    "print(\"Adding Known Good Queries to the Vector store.....\")\n",
    "# Add KGQ to the vector store\n",
    "if VECTOR_STORE=='bigquery-vector':\n",
    "    await(store_kgq_embeddings(df_kgq,\n",
    "                                project_id=PROJECT_ID,\n",
    "                                instance_name=None,\n",
    "                                database_name=None,\n",
    "                                schema=BQ_OPENDATAQNA_DATASET_NAME,\n",
    "                                database_user=None,\n",
    "                                database_password=None,\n",
    "                                region=BQ_DATASET_REGION,\n",
    "                                VECTOR_STORE = VECTOR_STORE\n",
    "                                ))\n",
    "\n",
    "elif VECTOR_STORE=='cloudsql-pgvector':\n",
    "    await(store_kgq_embeddings(df_kgq,\n",
    "                                project_id=PROJECT_ID,\n",
    "                                instance_name=PG_INSTANCE,\n",
    "                                database_name=PG_DATABASE,\n",
    "                                schema=None,\n",
    "                                database_user=PG_USER,\n",
    "                                database_password=PG_PASSWORD,\n",
    "                                region=PG_REGION,\n",
    "                                VECTOR_STORE = VECTOR_STORE\n",
    "                                ))\n",
    "print('Done!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talktodata-Fy2pM2BF-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
